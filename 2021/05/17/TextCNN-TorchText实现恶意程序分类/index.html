<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TextCNN+TorchText实现恶意程序分类"><meta name="keywords" content="机器学习,恶意程序分析"><meta name="author" content="Alston"><meta name="copyright" content="Alston"><title>TextCNN+TorchText实现恶意程序分类 | Alston's blog</title><link rel="shortcut icon" href="/1231489.png"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-数据格式"><span class="toc-text"> 1 数据格式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-加载数据"><span class="toc-text"> 2 加载数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-加载数据"><span class="toc-text"> 2.1 加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-划分训练集"><span class="toc-text"> 2.2 划分训练集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-textcnn-模型"><span class="toc-text"> 3 TextCNN 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-模型训练"><span class="toc-text"> 4 模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text"> 参考</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://pic4.zhimg.com/80/v2-1b0d0240350e3d8e0550845b6bdaad1e_hd.jpg"></div><div class="author-info__name text-center">Alston</div><div class="author-info__description text-center">计算机硕士在读</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">52</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">29</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">13</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://blog.csdn.net/Sc0fie1d" target="_blank" rel="noopener">Alston's CSDN</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s3.ax1x.com/2021/03/14/60DEdJ.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Alston's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">TextCNN+TorchText实现恶意程序分类</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-05-17</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.5k</span><span class="post-meta__separator">|</span><span>阅读时长: 10 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文介绍如何使用TextCNN实现恶意程序的分类任务。实验数据来自天池新人赛<a href="https://tianchi.aliyun.com/competition/entrance/231694/introduction" target="_blank" rel="noopener">阿里云安全恶意程序检测</a>，整个比赛实现了多种模型，最终通过模型融合实现分类任务，TextCNN为所用模型之一。<strong>本文实现了使用torchtext直接从列表加载和处理数据，设计了k-fold cross validation进行交叉验证，并使用torch Conv1d实现textcnn。</strong></p>
<h2 id="1-数据格式"><a class="markdownIt-Anchor" href="#1-数据格式"></a> 1 数据格式</h2>
<p>通过对原始数据集进行处理之后，训练集和测试集分别用 .plk 文件保存在本地。训练集内容包括特征（恶意程序的API调用序列，每个序列是一个以空格分隔的字符串）和标签，均存储在list中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../dataset/security_train.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    labels = pickle.load(f)</span><br><span class="line">    train_api = pickle.load(f)</span><br><span class="line">print(labels[<span class="number">0</span>])</span><br><span class="line">print(train_api[<span class="number">0</span>][<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line">==============output==============</span><br><span class="line"><span class="number">5</span></span><br><span class="line">LdrLoadDll LdrGetProcedureAddress LdrGetProcedureAddress...</span><br></pre></td></tr></table></figure>
<p>测试集内容包括文件id（恶意程序）和特征，均存储在list中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../dataset/security_test.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    file_ids = pickle.load(f)</span><br><span class="line">    test_api = pickle.load(f)</span><br><span class="line">print(file_ids[<span class="number">0</span>])</span><br><span class="line">print(test_api[<span class="number">0</span>][<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line">==============output==============</span><br><span class="line"><span class="number">1</span></span><br><span class="line">RegOpenKeyExA CopyFileA OpenSCManagerA CreateServiceA...</span><br></pre></td></tr></table></figure>
<h2 id="2-加载数据"><a class="markdownIt-Anchor" href="#2-加载数据"></a> 2 加载数据</h2>
<p>这里我们使用文本处理工具 <strong>torchtext</strong> 来进行数据的加载和预处理。不同于从 .csv 文件加载数据的常规做法，我们直接从上述列表中加载数据。<strong>此外，我们还借助 sklearn 在加载数据过程中实现了对训练集的划分，便于后续的交叉验证。</strong></p>
<h3 id="21-加载数据"><a class="markdownIt-Anchor" href="#21-加载数据"></a> <strong>2.1 加载数据</strong></h3>
<p>在 torchtext 中，所有 DataSet 对象中的数据都是一个 <strong>examples</strong> 列表（可以通过<strong>DataSet.examples</strong> 获得），而该列表中的每一条数据为一个 <strong>example 对象</strong>。因此，我们可以<strong>使用 data.Example 中的 fromlist 方法，来从列表中构建数据集。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, seed=<span class="number">1234</span>, test=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            print(<span class="string">"gpu cuda is available!"</span>)</span><br><span class="line">            torch.cuda.manual_seed(seed)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"cuda is not available! cpu is available!"</span>)</span><br><span class="line">            torch.manual_seed(seed)</span><br><span class="line">        self.seed = seed</span><br><span class="line">        self.train_examples = []</span><br><span class="line">        self.test_examples = []</span><br><span class="line">        <span class="comment"># 分词器</span></span><br><span class="line">        self.tokenizer = <span class="keyword">lambda</span> x: x.split()</span><br><span class="line">        <span class="comment"># 定义如何处理特征字段和标签字段</span></span><br><span class="line">        self.TEXT = data.Field(sequential=<span class="literal">True</span>, tokenize=self.tokenizer, fix_length=<span class="number">8000</span>)</span><br><span class="line">        self.LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># file_id, api_seq, label为每个example中的字段（如exampl.label）</span></span><br><span class="line">        fields = [(<span class="string">'file_id'</span>, <span class="literal">None</span>), </span><br><span class="line">                  (<span class="string">'api_seq'</span>, self.TEXT), </span><br><span class="line">                  (<span class="string">'label'</span>, self.LABEL)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载数据</span></span><br><span class="line">        <span class="keyword">if</span> test:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'../dataset/security_test.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                file_ids = pickle.load(f)</span><br><span class="line">                test_api = pickle.load(f)</span><br><span class="line">            <span class="keyword">for</span> f_id, tes_api <span class="keyword">in</span> zip(file_ids, test_api):</span><br><span class="line">                <span class="comment"># fields中的三项分别对应列表中的三项</span></span><br><span class="line">                self.test_examples.append(Example.fromlist([f_id, tes_api, <span class="literal">None</span>], fields))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'../dataset/security_train.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                labels = pickle.load(f)</span><br><span class="line">                train_api = pickle.load(f)</span><br><span class="line">            <span class="keyword">for</span> tra_api, label <span class="keyword">in</span> zip(train_api, labels):</span><br><span class="line">                self.train_examples.append(Example.fromlist([<span class="literal">None</span>, tra_api, label], fields))</span><br></pre></td></tr></table></figure>
<h3 id="22-划分训练集"><a class="markdownIt-Anchor" href="#22-划分训练集"></a> 2.2 划分训练集</h3>
<p>对训练集进行划分，便于后的的交叉验证：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fold_data</span><span class="params">(self, num_folds=<span class="number">5</span>)</span>:</span></span><br><span class="line">    fields = [(<span class="string">'api_seq'</span>, self.TEXT), (<span class="string">'label'</span>, self.LABEL)]</span><br><span class="line">    </span><br><span class="line">    kf = KFold(n_splits=num_folds, shuffle=<span class="literal">True</span>, random_state=self.seed)</span><br><span class="line">    train_data_arr = np.array(self.train_examples)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> train_index, val_index <span class="keyword">in</span> kf.split(train_data_arr):</span><br><span class="line">        <span class="keyword">yield</span>(</span><br><span class="line">            self.TEXT,</span><br><span class="line">            self.LABEL,</span><br><span class="line">            data.Dataset(train_data_arr[train_index], fields=fields),</span><br><span class="line">            data.Dataset(train_data_arr[val_index], fields=fields),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_data</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.test_examples</span><br></pre></td></tr></table></figure>
<h2 id="3-textcnn-模型"><a class="markdownIt-Anchor" href="#3-textcnn-模型"></a> 3 TextCNN 模型</h2>
<p>TextCNN模型如下图所示，其过程非常简单。首先对一个 <strong>text_len * embedding_size</strong> 的矩阵做一维卷积，然后将卷积结果做max pooling后拼接，最后输入到全连接网络中进行分类。</p>
<p><img src="/2021/05/17/TextCNN-TorchText%E5%AE%9E%E7%8E%B0%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%86%E7%B1%BB/v2-d598c5a8a821f53c9f789dfdb5fc965b_b.jpeg" alt="TextCNN"></p>
<p>这里我们首先介绍一下torch中<strong>Conv1d</strong>和<strong>Conv2d</strong>：</p>
<ul>
<li><strong>Conv1d</strong></li>
</ul>
<p><strong>Conv1d表示卷积的方向是一维的</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">Conv1d</span><span class="params">(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=True)</span></span></span><br><span class="line"><span class="class">#<span class="title">in_channels</span><span class="params">(int)</span> 输入信号的通道。在文本分类中，即为词向量的维度；图像中为<span class="title">RGB</span>三通道。</span></span><br><span class="line"><span class="class"># <span class="title">out_channels</span><span class="params">(int)</span> 卷积产生的通道。有多少个<span class="title">out_channels</span>，就需要多少个1维卷积 </span></span><br><span class="line"><span class="class"># <span class="title">kernel_size</span><span class="params">(int or tuple)</span> 卷积核的尺寸，卷积核的大小 </span></span><br><span class="line"><span class="class"># <span class="title">stride</span><span class="params">(int or tuple, optional)</span> 卷积步长 </span></span><br><span class="line"><span class="class"># <span class="title">padding</span> <span class="params">(int or tuple, optional)</span> 输入的每一条边补充0的层数 </span></span><br><span class="line"><span class="class"># <span class="title">dilation</span><span class="params">(int or tuple, `optional``)</span> 空洞卷积，卷积核元素之间的间距 </span></span><br><span class="line"><span class="class"># <span class="title">groups</span><span class="params">(int, optional)</span> 从输入通道到输出通道的阻塞连接数 </span></span><br><span class="line"># bias(bool, optional) 如果bias=True，添加偏置</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>in_channels(int)</strong>：输入信号的通道。在文本分类中，即为词向量的维度； <strong>out_channels(int)</strong>：卷积产生的通道，即filter的数量 <strong>kernel_size(int or tuple)</strong>： 卷积核的尺寸。kernel_size为int时，其大小kernel_size*in_channels；kernel_size为tuple时，其大小 kernel_size[0]*kernel_size[1] <strong>stride(int or tuple, optional)</strong> ： 卷积步长  <strong>padding (int or tuple, optional)</strong>：输入的每一条边补充0的层数  <strong>dilation(int or tuple, `optional``)</strong> ：空洞卷积，卷积核元素之间的间距 <strong>groups(int, optional)</strong> ：从输入通道到输出通道的阻塞连接数  <strong>bias(bool, optional)</strong> ： 如果bias=True，添加偏置</p>
</blockquote>
<ul>
<li><strong>Conv2d</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">Conv2d</span><span class="params">(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=True)</span></span></span><br></pre></td></tr></table></figure>
<p>Conv2d与Conv1d的参数完全相同，只不过其用在图像卷积中，含义与文本卷积不同。</p>
<blockquote>
<p><strong>in_channels</strong>：通道数，图像中为RGB三通道。channel本质上定义了一个数据点用几个值来描述 <strong>out_channels</strong>：同样是filter的数量，也即输出后的通道数 <strong>kernel_size</strong>：这里的kernel size可以是int或tuple的，如kernel_size=2，那么就是2*2大小的卷积核；如果是(2,3)，那么就是一个non-squre的卷积核，相应的stride和padding也可以设置为tuple</p>
</blockquote>
<p><strong>所以这里我们可以使用Conv1d和Conv2d实现textcnn。如果使用Conv2d，那么要设置in_channels=1，kernel_size设置为tuple，kernel_size[0]为Conv1d中kernel_size的大小，而kernel_size[1]为Conv1d中in_channels的大小。</strong>（不明白为什么那么多教程用Conv2d实现textcnn）</p>
<ul>
<li><strong>Model</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> th</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TextCNN, self).__init__()</span><br><span class="line">        vocab = <span class="number">304</span></span><br><span class="line">        dim = <span class="number">256</span></span><br><span class="line">        class_num = <span class="number">8</span></span><br><span class="line">        filters = <span class="number">64</span></span><br><span class="line">        kernels = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        dilations = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">        self.embed = nn.Embedding(num_embeddings=vocab, embedding_dim=dim)</span><br><span class="line">        self.dp1 = nn.Dropout2d(<span class="number">0.25</span>)</span><br><span class="line">        self.convs = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> ks <span class="keyword">in</span> kernels:</span><br><span class="line">            <span class="keyword">for</span> dila <span class="keyword">in</span> dilations:</span><br><span class="line">                self.convs.append(nn.Conv1d(in_channels=dim, out_channels=filters, kernel_size=ks, dilation=dila))</span><br><span class="line">        self.conv_ac = nn.ReLU()</span><br><span class="line">        <span class="comment"># MaxPooling，在最后一维的行上做最大池化，1为输出的size</span></span><br><span class="line">        self.pool1 = nn.AdaptiveMaxPool1d(<span class="number">1</span>)</span><br><span class="line">        self.dp2 = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">256</span>)</span><br><span class="line">        self.ac1 = nn.ReLU()</span><br><span class="line">        self.dp3 = nn.Dropout(<span class="number">0.3</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 输出后的x.shape = (batch_size * text_len * embedding_size) </span></span><br><span class="line">        x = self.embed(x) </span><br><span class="line">        x = self.dp1(x)</span><br><span class="line">        <span class="comment"># Conv1d对输入数据的最后一维进行一维卷积</span></span><br><span class="line">        <span class="comment"># 输出后的x.shape = (batch_size * embedding_size * text_len)</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>) </span><br><span class="line">        warppers = []</span><br><span class="line">        <span class="keyword">for</span> conv1d <span class="keyword">in</span> self.convs:</span><br><span class="line">            <span class="comment"># 输出shape：batch_size * out_channels(64) * W(&lt;=8000) </span></span><br><span class="line">            conv = conv1d(x)</span><br><span class="line">            conv_ac = self.conv_ac(conv)</span><br><span class="line">            <span class="comment"># without permute</span></span><br><span class="line">            <span class="comment"># 最后一维做最大池化，指定池化后的输出size为1</span></span><br><span class="line">            <span class="comment"># pool shape：batch_size * out_channels(64) * 1</span></span><br><span class="line">            pool = self.pool1(conv_ac)</span><br><span class="line">            warppers.append(pool.squeeze(<span class="number">-1</span>))</span><br><span class="line">        <span class="comment"># 所有的卷积结果拼接成一个向量：shape：batch_size * 1024(64*16)</span></span><br><span class="line">        x = th.cat(warppers, dim=<span class="number">-1</span>)</span><br><span class="line">        x = self.dp2(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.ac1(x)</span><br><span class="line">        x = self.dp3(x)</span><br><span class="line">        rst = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> rst</span><br></pre></td></tr></table></figure>
<h2 id="4-模型训练"><a class="markdownIt-Anchor" href="#4-模型训练"></a> 4 模型训练</h2>
<p>在我们的数据中，假设每个词对应的词向量维度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi><mi>i</mi><mi>m</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">Dim=256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span></span></span></span> ，每一个样本的分词后的长度已知设为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mn>8000</mn></mrow><annotation encoding="application/x-tex">W = 8000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span></span></span></span> （在加载数据时的TEXT=data.Field()中的fix_length参数指定），每个 mini-batch 的大小为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">N=32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">2</span></span></span></span> 。那么我们希望得到的是一个个维度为$N ∗ W ∗ D i m $的浮点数数据作为卷积层的输入</p>
<p>在数据加载时，我们已经实现了<strong>分词</strong>，且无需<strong>去除停用词</strong>，因此我们加载完数据后首先使用TEXT.build_vocab 建立词汇表（词汇表是词语到 index 的映射，index从0到M，M为已知词汇的个数，如{'我‘:0, ‘好’:1, ‘帅’：2…}）。</p>
<p>然后我们调用 <strong>data.BucketIterator</strong> 将产生 Iterator 迭代数据，并指定batch_size，此时Iterator 中每个 batch 的大小即为 <strong>batch_size * text_len (32*8000)</strong>。将数据输入到模型中，经过<strong>nn.Embedding</strong>层之后，其shape变为  <strong>batch_size * text_len * embedding size (32*8000*256)，<strong>即卷积层的输入</strong>。</strong></p>
<p>由于Torch没有实现cross-validation，因此我们用自定义的data_loader，在训练过程中使用5折交叉验证，即80%的数据训练，20%的数据验证，交叉验证五轮。训练与验证过程的main函数如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data_generator = MyDataset()</span><br><span class="line">    _history = []</span><br><span class="line">    device = <span class="literal">None</span></span><br><span class="line">    model = <span class="literal">None</span></span><br><span class="line">    fold_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> TEXT, LABEL, train_data, val_data <span class="keyword">in</span> data_generator.get_fold_data():</span><br><span class="line">        logger.info(<span class="string">"[+] Running Training ..."</span>)</span><br><span class="line">        logger.info(<span class="string">f"Now fold: <span class="subst">&#123;fold_index + <span class="number">1</span>&#125;</span> / <span class="subst">&#123;<span class="number">5</span>&#125;</span>"</span>)  <span class="comment"># num_folds=5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建词汇表（只在train_data上构建即可）</span></span><br><span class="line">        TEXT.build_vocab(train_data)</span><br><span class="line">        LABEL.build_vocab(train_data)  <span class="comment"># For converting str into float labels.</span></span><br><span class="line"></span><br><span class="line">        model = TextCNN()</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.00006</span>, weight_decay=<span class="number">0.00006</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># set cuda</span></span><br><span class="line">        device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">        model = model.to(device)</span><br><span class="line"></span><br><span class="line">        train_iterator = data.BucketIterator(train_data, batch_size=<span class="number">32</span>, sort_key=<span class="keyword">lambda</span> x: len(x.api_seq), device=device)</span><br><span class="line">        val_iterator = data.BucketIterator(val_data, batch_size=<span class="number">32</span>, sort_key=<span class="keyword">lambda</span> x: len(x.api_seq), device=device)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">80</span>):</span><br><span class="line">            train_loss, train_acc = train_run(model, train_iterator, optimizer, device)</span><br><span class="line">            val_loss, val_acc = eval_run(model, val_iterator, device)</span><br><span class="line">            logger.info(<span class="string">f'| Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:<span class="number">02</span>&#125;</span> | Train Loss: <span class="subst">&#123;train_loss:<span class="number">.6</span>f&#125;</span> | Train ACC: <span class="subst">&#123;train_acc:<span class="number">.6</span>f&#125;</span> '</span></span><br><span class="line">                        <span class="string">f'| Val Loss: <span class="subst">&#123;val_loss:<span class="number">.6</span>f&#125;</span> | Val ACC: <span class="subst">&#123;val_acc:<span class="number">.6</span>f&#125;</span>'</span>)</span><br><span class="line">        best = model</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">f"best_model_<span class="subst">&#123;fold_index&#125;</span>.pkl"</span>)</span><br><span class="line">        fold_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>train_run() 和 eval_run() 会返回训练集和测试集上的loss和accuracy，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_run</span><span class="params">(model, iterator, optimizer, device)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        model.train()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># shape: text_len * batch_size ==&gt; batch_size * text_len</span></span><br><span class="line">        feat = batch.api_seq.t_()</span><br><span class="line">        output = model(feat)</span><br><span class="line">        target = batch.label</span><br><span class="line">        feat, target = feat.to(device), target.to(device)</span><br><span class="line">        <span class="comment"># shape: output([batch_size, 8]); target([batch_size])</span></span><br><span class="line">        loss = F.cross_entropy(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        <span class="comment"># [0]为最大值，[1]为最大值对应的index</span></span><br><span class="line">        result = torch.max(output, <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        corrects += (result.view(target.size()).data == target.data).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), corrects / (len(iterator) * <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_run</span><span class="params">(model, iterator, device)</span>:</span></span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            model.eval()</span><br><span class="line">            feat = batch.api_seq.t_()</span><br><span class="line">            predictions = model(feat)</span><br><span class="line">            target = batch.label</span><br><span class="line">            feat, target = feat.to(device), target.to(device)</span><br><span class="line">            loss = F.cross_entropy(predictions, target)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            result = torch.max(predictions, <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            corrects += (result.view(target.size()).data == target.data).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), corrects / (len(iterator) * <span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p>每次训练结束后，保存最优模型，最后将5个模型在test_data上的输出取平均，即最终结果。</p>
<p>本实验过程仅在测试集数据上做了交叉验证，在真正比赛时，需要在所有数据上构建词典（TEXT.build_vocab(train_data, test_data)），以保证模型在测试集上的精度。</p>
<p>此外，在代码运行过程中，GPU并没有跑满，显存占用8G/10G，GPU利用率80%，内存占满。瓶颈之一是内存不够(16G)，另外textcnn的data.BucketIterator不像Dataloader一样可以配置num_workers实现多线程，导致性能没有发挥到最佳。</p>
<h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2>
<p>[1]：<a href="https://blog.csdn.net/nlpuser/article/details/88067167" target="_blank" rel="noopener">TorchText用法示例及完整代码_nlpuser的博客-CSDN博客_torchtext</a></p>
<p>[2]：<a href="https://blog.csdn.net/qq_25037903/article/details/85058217" target="_blank" rel="noopener">pytorch实现textCNN_无所知的博客-CSDN博客</a></p>
<p>[3]：<a href="https://zhuanlan.zhihu.com/p/79734775" target="_blank" rel="noopener">pytorch TextCNN笔记</a></p>
<p>[4]：<a href="https://xz.aliyun.com/t/3704#toc-5" target="_blank" rel="noopener">用机器学习进行恶意软件检测–以阿里云恶意软件检测比赛为例 - 先知社区</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Alston</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lizitong67.github.io/2021/05/17/TextCNN-TorchText%E5%AE%9E%E7%8E%B0%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%86%E7%B1%BB/">https://lizitong67.github.io/2021/05/17/TextCNN-TorchText%E5%AE%9E%E7%8E%B0%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%86%E7%B1%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lizitong67.github.io">Alston's blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/">恶意程序分析</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2021/05/17/%E7%BD%91%E7%BB%9C%E5%A8%81%E8%83%81%E6%83%85%E6%8A%A5%E4%B9%8BSTIX2-1/"><span>网络威胁情报之STIX2.1</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://s3.ax1x.com/2021/03/14/60DEdJ.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By Alston</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>