<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Python3爬虫笔记-urllib"><meta name="keywords" content="Python,爬虫"><meta name="author" content="Alston"><meta name="copyright" content="Alston"><title>Python3爬虫笔记-urllib | Alston's blog</title><link rel="shortcut icon" href="/1231489.png"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-urllibrequest发送请求"><span class="toc-text"> 1 urllib.request发送请求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-urllibrequest高级用法"><span class="toc-text"> 1.1 urllib.request高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#111-验证"><span class="toc-text"> 1.1.1 验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#112-代理"><span class="toc-text"> 1.1.2 代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#113-cookies"><span class="toc-text"> 1.1.3 Cookies</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-urlliberror处理异常"><span class="toc-text"> 2 urllib.error处理异常</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-urlerror"><span class="toc-text"> 2.1 URLError</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-httperror"><span class="toc-text"> 2.2 HTTPError</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-urllibparse解析链接"><span class="toc-text"> 3 urllib.parse解析链接</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#31-urlparse"><span class="toc-text"> 3.1 urlparse()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-urlunparse"><span class="toc-text"> 3.2 urlunparse()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#33-urlsplit"><span class="toc-text"> 3.3 urlsplit()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#34-urlunsplit"><span class="toc-text"> 3.4 urlunsplit()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#35-urljoin"><span class="toc-text"> 3.5 urljoin()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#36-urlencode"><span class="toc-text"> 3.6 urlencode()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#37-parse_qs"><span class="toc-text"> 3.7 parse_qs()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#38-parse_qsl"><span class="toc-text"> 3.8 parse_qsl()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#39-quote"><span class="toc-text"> 3.9 quote()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#310-unquote"><span class="toc-text"> 3.10 unquote()</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-urllibrobotparser分析robots协议"><span class="toc-text"> 4 urllib.robotparser分析Robots协议</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#41-常见搜索爬虫的名称及其对应的网站"><span class="toc-text"> 4.1 常见搜索爬虫的名称及其对应的网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#42-robotparser"><span class="toc-text"> 4.2 robotparser</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://pic4.zhimg.com/80/v2-1b0d0240350e3d8e0550845b6bdaad1e_hd.jpg"></div><div class="author-info__name text-center">Alston</div><div class="author-info__description text-center">计算机硕士在读</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">46</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">12</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://blog.csdn.net/Sc0fie1d" target="_blank" rel="noopener">Alston's CSDN</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s2.ax1x.com/2020/02/21/3uWnXD.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Alston's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">Python3爬虫笔记-urllib</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Python/">Python</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><ul>
<li>urllib库是Python内置的HTTP请求库，不需要额外安装。它包含如下4个模块：
<ul>
<li>request：HTTP请求模块</li>
<li>error：异常处理模块</li>
<li>parse：提供URL处理方法，包括拆分、解析、合并等</li>
<li>robotparser：识别网站等robot.txt文件</li>
</ul>
</li>
</ul>
<h1 id="1-urllibrequest发送请求"><a class="markdownIt-Anchor" href="#1-urllibrequest发送请求"></a> 1 urllib.request发送请求</h1>
<ul>
<li>连接URL，获取返回页面的源代码；默认请求方式为GET</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">print</span> (response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#response是一个HttpResponded对象，其中包含许多方法</span></span><br><span class="line"><span class="keyword">print</span> (type(response)) 	</span><br><span class="line"><span class="keyword">print</span> (response.getheaders())</span><br><span class="line"><span class="keyword">print</span> (response.getheader(<span class="string">'Server'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>使用POST方法提交数据使用了data参数之后，请求方式就变成了POST</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#byte()方法将数据转化为字节流，其中第一个参数将字典转化为字符串</span></span><br><span class="line">data = byte(urllib.parse.urlencode(&#123;<span class="string">'word'</span>:<span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br></pre></td></tr></table></figure>
<ul>
<li>设定超时时间</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket </span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">	<span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">		<span class="keyword">print</span> (<span class="string">'TIME OUT'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Request：使用Request，将请求独立成一个对象，还可以更加丰富灵活的配置参数。<br />
此时data参数必须要上传bytes类型。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = urllib.request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = urllib.request.urlopen(req)</span><br><span class="line"></span><br><span class="line"><span class="comment">#另外，headers参数也可以用add_headers()方法来添加</span></span><br><span class="line">req = urllib.request.Request(url=url, data=data, method=<span class="string">'POST'</span>)</span><br><span class="line">req.add_headers(<span class="string">'User-Agent'</span> ,<span class="string">'xxxxxxxxxx'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="11-urllibrequest高级用法"><a class="markdownIt-Anchor" href="#11-urllibrequest高级用法"></a> 1.1 urllib.request高级用法</h2>
<ul>
<li>利用Handler来创建Opener</li>
</ul>
<h3 id="111-验证"><a class="markdownIt-Anchor" href="#111-验证"></a> 1.1.1 验证</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<h3 id="112-代理"><a class="markdownIt-Anchor" href="#112-代理"></a> 1.1.2 代理</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<h3 id="113-cookies"><a class="markdownIt-Anchor" href="#113-cookies"></a> 1.1.3 Cookies</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意，获取到的cookie存储在CookieJar对象中</span></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+ <span class="string">"="</span> +item.value)</span><br><span class="line"></span><br><span class="line"><span class="comment">#cookie以MozillaCookieJar格式保存在文本文档中</span></span><br><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#从文本文档中读取LWPCookieJar格式的cookie</span></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">'cookies.txt'</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h1 id="2-urlliberror处理异常"><a class="markdownIt-Anchor" href="#2-urlliberror处理异常"></a> 2 urllib.error处理异常</h1>
<p>urllib的<kbd>error</kbd>模块定义了由<kbd>request</kbd>模块产生的异常。如果出现了问题，<kbd>request</kbd>模块便会抛出<kbd>error</kbd>模块中定义的异常。</p>
<h2 id="21-urlerror"><a class="markdownIt-Anchor" href="#21-urlerror"></a> 2.1 URLError</h2>
<p>由request模块生的异常都可以通过捕获这个类来处理。它具有一个属性reason，即返回错误的原因。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<h2 id="22-httperror"><a class="markdownIt-Anchor" href="#22-httperror"></a> 2.2 HTTPError</h2>
<p>它是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。它有如下3个属性:</p>
<ul>
<li><kbd>code</kbd>：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。</li>
<li><kbd>reason</kbd>：同父类一样，用于返回错误的原因。</li>
<li><kbd>headers</kbd>：返回请求头。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>因为URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="3-urllibparse解析链接"><a class="markdownIt-Anchor" href="#3-urllibparse解析链接"></a> 3 urllib.parse解析链接</h1>
<h2 id="31-urlparse"><a class="markdownIt-Anchor" href="#31-urlparse"></a> 3.1 urlparse()</h2>
<ul>
<li>该方法可以实现URL的识别和分段：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(type(result), result)</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">urllib</span>.<span class="title">parse</span>.<span class="title">ParseResult</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">ParseResult</span><span class="params">(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>urlparse()的API用法：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><kbd>urlstring</kbd>：这是必填项，即待解析的URL。</p>
<p><kbd>scheme</kbd>：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议；假如链接带有协议信息，则这个参数不能更改原链接的协议。</p>
<p><kbd>allow_fragments</kbd>：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、parameters或者query的一部分，而fragment部分为空。</p>
<ul>
<li>返回结果ParseResult实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>], result.netloc, result[<span class="number">1</span>], sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="32-urlunparse"><a class="markdownIt-Anchor" href="#32-urlunparse"></a> 3.2 urlunparse()</h2>
<ul>
<li>urlunparse()接受的参数是一个可迭代对象，但是它的长度必须是6，否则会抛出参数数量不足或者过多的问题</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"> </span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">http://www.baidu.com/index.html;user?a=6#comment</span><br></pre></td></tr></table></figure>
<h2 id="33-urlsplit"><a class="markdownIt-Anchor" href="#33-urlsplit"></a> 3.3 urlsplit()</h2>
<ul>
<li>这个方法和urlparse()方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中。</li>
<li>返回结果是SplitResult，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取。</li>
</ul>
<h2 id="34-urlunsplit"><a class="markdownIt-Anchor" href="#34-urlunsplit"></a> 3.4 urlunsplit()</h2>
<ul>
<li>与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为5。</li>
</ul>
<h2 id="35-urljoin"><a class="markdownIt-Anchor" href="#35-urljoin"></a> 3.5 urljoin()</h2>
<ul>
<li>我们可以提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果。</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html?question=2'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com?wd=abc'</span>, <span class="string">'https://cuiqingcai.com/index.php'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com#comment'</span>, <span class="string">'?category=2'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">http://www.baidu.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html?question=2</span><br><span class="line">https://cuiqingcai.com/index.php</span><br><span class="line">http://www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2</span><br></pre></td></tr></table></figure>
<p>可以发现，base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的。</p>
<h2 id="36-urlencode"><a class="markdownIt-Anchor" href="#36-urlencode"></a> 3.6 urlencode()</h2>
<p>urlencode()在构造GET请求参数的时候非常有用</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure>
<h2 id="37-parse_qs"><a class="markdownIt-Anchor" href="#37-parse_qs"></a> 3.7 parse_qs()</h2>
<p>有了序列化，必然就有反序列化。如果我们有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"> </span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">&#123;<span class="string">'name'</span>: [<span class="string">'germey'</span>], <span class="string">'age'</span>: [<span class="string">'22'</span>]&#125;</span><br></pre></td></tr></table></figure>
<h2 id="38-parse_qsl"><a class="markdownIt-Anchor" href="#38-parse_qsl"></a> 3.8 parse_qsl()</h2>
<p>用于将参数转化为元组组成的列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line"> </span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qsl(query))</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">[(<span class="string">'name'</span>, <span class="string">'germey'</span>), (<span class="string">'age'</span>, <span class="string">'22'</span>)]</span><br></pre></td></tr></table></figure>
<h2 id="39-quote"><a class="markdownIt-Anchor" href="#39-quote"></a> 3.9 quote()</h2>
<ul>
<li>将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码:</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"> </span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure>
<h2 id="310-unquote"><a class="markdownIt-Anchor" href="#310-unquote"></a> 3.10 unquote()</h2>
<ul>
<li>进行URL解码</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line"> </span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'</span></span><br><span class="line">print(unquote(url))</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">https://www.baidu.com/s?wd=壁纸</span><br></pre></td></tr></table></figure>
<h1 id="4-urllibrobotparser分析robots协议"><a class="markdownIt-Anchor" href="#4-urllibrobotparser分析robots协议"></a> 4 urllib.robotparser分析Robots协议</h1>
<ul>
<li>
<p>Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。</p>
</li>
<li>
<p>当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。</p>
</li>
</ul>
<p>下面的例子实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br><span class="line">Allow: &#x2F;public&#x2F;</span><br></pre></td></tr></table></figure>
<p>上面的User-agent描述了搜索爬虫的名称，这里将其设置为*则代表该协议对任何爬取爬虫有效。比如，我们可以设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">User-agent: Baiduspider</span><br></pre></td></tr></table></figure>
<p>这就代表我们设置的规则对百度爬虫是有效的。如果有多条User-agent记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
<ul>
<li>下面是几个例子：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#禁止所有爬虫访问任何目录的代码如下：</span><br><span class="line">User-agent: * </span><br><span class="line">Disallow: &#x2F;</span><br><span class="line"></span><br><span class="line">#允许所有爬虫访问任何目录的代码如下：</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow:</span><br><span class="line"></span><br><span class="line">#直接把robots.txt文件留空也是可以的。</span><br><span class="line"></span><br><span class="line">#禁止所有爬虫访问网站某些目录的代码如下：</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;private&#x2F;</span><br><span class="line">Disallow: &#x2F;tmp&#x2F;</span><br><span class="line"></span><br><span class="line">#只允许某一个爬虫访问的代码如下：</span><br><span class="line">User-agent: WebCrawler</span><br><span class="line">Disallow:</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: &#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="41-常见搜索爬虫的名称及其对应的网站"><a class="markdownIt-Anchor" href="#41-常见搜索爬虫的名称及其对应的网站"></a> 4.1 常见搜索爬虫的名称及其对应的网站</h2>
<p><img src="https://img-blog.csdnimg.cn/20191019202610869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NjMGZpZTFk,size_16,color_FFFFFF,t_70#pic_center" alt="" /></p>
<h2 id="42-robotparser"><a class="markdownIt-Anchor" href="#42-robotparser"></a> 4.2 robotparser</h2>
<ul>
<li>可以使用robotparser模块来解析robots.txt。该模块提供了一个类<kbd>RobotFileParser</kbd>，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">urllib.robotparser.RobotFileParser(url=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<p>下面列出了这个类常用的几个方法。</p>
<ul>
<li><kbd>set_url()</kbd>：用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。</li>
<li><kbd>read()</kbd>：读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。</li>
<li><kbd>parse()</kbd>：用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li>
<li><kbd>can_fetch()</kbd>：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。</li>
<li><kbd>mtime()</kbd>：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。</li>
<li><kbd>modified()</kbd>：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"> </span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以在声明RobotFileParser时直接set_url：</span></span><br><span class="line">rp = RobotFileParser(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Alston</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lizitong67.github.io/2020/02/21/Python3%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0-urllib/">https://lizitong67.github.io/2020/02/21/Python3%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0-urllib/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lizitong67.github.io">Alston's blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/02/21/Python3%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0-requests/"><i class="fa fa-chevron-left">  </i><span>Python3爬虫笔记-requests</span></a></div><div class="next-post pull-right"><a href="/2020/02/21/Python%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-UDP/"><span>Python网络编程-UDP</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://s2.ax1x.com/2020/02/21/3uWnXD.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By Alston</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" data-click="false"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>